{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from matplotlib.colors import ListedColormap\n",
                "from sklearn import svm, neighbors, tree, ensemble, model_selection, metrics\n",
                "from sklearn.inspection import DecisionBoundaryDisplay\n",
                "from imblearn.over_sampling import SMOTE\n",
                "from imblearn.under_sampling import NearMiss, RandomUnderSampler\n",
                "\n",
                "sns.set_theme()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_folder = \"data/traces/\"\n",
                "\n",
                "def read_date(date: str):\n",
                "    \"\"\"Aggregate data of a single from parts\"\"\"\n",
                "    data = pd.DataFrame()\n",
                "\n",
                "    for i in range(50):\n",
                "        try:\n",
                "            frame = pd.read_csv(data_folder + date + \"-{}.csv\".format(i))\n",
                "            data = pd.concat([data, frame], axis=0)\n",
                "        except OSError:\n",
                "            pass\n",
                "\n",
                "    return data\n",
                "\n",
                "\n",
                "def read_date_part(date_part: str):\n",
                "    \"\"\"Read data of a part of a capture date\"\"\"\n",
                "    data = pd.DataFrame()\n",
                "\n",
                "    try:\n",
                "        frame = pd.read_csv(data_folder + date_part + \".csv\")\n",
                "        data = pd.concat([data, frame], axis=0)\n",
                "    except OSError:\n",
                "        pass\n",
                "\n",
                "    return data\n",
                "\n",
                "data = read_date_part(\"16-09-23\")\n",
                "data.set_index(\"timestamp\")\n",
                "\n",
                "display(data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def attach_window_id(timestamped_frame: pd.DataFrame) -> list[pd.DataFrame]:\n",
                "    \"\"\"Slice the dataframe into 1-second intervals\"\"\"\n",
                "    first_packet_time = timestamped_frame.iloc[0]['timestamp']\n",
                "    last_packet_time = timestamped_frame.iloc[-1]['timestamp']\n",
                "    windowed_frame = timestamped_frame.set_index('timestamp', drop=False) \n",
                "\n",
                "    ns_in_sec = int(1e9)\n",
                "    number_of_windows = int((last_packet_time - first_packet_time) / ns_in_sec) + 1\n",
                "\n",
                "    try:\n",
                "        windowed_frame.insert(0, value=0, column=\"window_id\")\n",
                "    except Exception as e:\n",
                "        print(\"It seems like the column 'window_id' already exists. Just updating values...\")\n",
                "        print(e)\n",
                "    finally:\n",
                "        windowed_frame.window_id.astype(np.int64)\n",
                "\n",
                "    for window_id in range(0, number_of_windows):\n",
                "        from_time = first_packet_time + window_id * ns_in_sec\n",
                "        to_time = first_packet_time + (window_id  + 1) * ns_in_sec\n",
                "        windowed_frame.loc[from_time:to_time, 'window_id'] = window_id\n",
                "    \n",
                "    windowed_frame.set_index('window_id', inplace=True)\n",
                "    return windowed_frame\n",
                "\n",
                "\n",
                "def attach_window_id_fast(timestamped_frame: pd.DataFrame) -> pd.DataFrame:\n",
                "    first_packet_time = timestamped_frame.iloc[0]['timestamp']\n",
                "    last_packet_time = timestamped_frame.iloc[-1]['timestamp']\n",
                "\n",
                "    ns_in_sec = int(1e9)\n",
                "    windows = range(first_packet_time - 1, last_packet_time + ns_in_sec, ns_in_sec)\n",
                "\n",
                "    windowed_frame = pd.cut(timestamped_frame['timestamp'], windows, labels=range(len(windows) - 1)).to_frame()\n",
                "    windowed_frame.columns = [\"window_id\"]\n",
                "    return pd.concat([timestamped_frame, windowed_frame], axis=1).set_index('window_id')\n",
                "    \n",
                "\n",
                "devices = pd.read_csv(\"data/list_of_devices.csv\")\n",
                "windowed_frame = attach_window_id_fast(data)\n",
                "\n",
                "display(windowed_frame)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_features_labeled(windowed_frame: pd.DataFrame, device_frame: pd.DataFrame, padded=False):\n",
                "    \"\"\"Calculate the mode, mean, and median of a window\"\"\"\n",
                "    if padded:\n",
                "        windowed_frame = pad_data(windowed_frame) \n",
                "\n",
                "    features = pd.DataFrame()\n",
                "\n",
                "    for device_index in device_frame.index:\n",
                "        device = device_frame.iloc[device_index]\n",
                "        device_packets = windowed_frame.loc[windowed_frame['eth_src'] == device['eth_src']]\n",
                "        device_features = generate_features(device_packets)\n",
                "\n",
                "        device_features.insert(len(device_features.columns), \"device_name\", device['device_name'])\n",
                "        device_features.insert(len(device_features.columns), \"iot\", device['iot'])\n",
                "\n",
                "        features = pd.concat([features, device_features])\n",
                "\n",
                "    features.columns = [\"mean\", \"std\", \"n_bytes\", \"device_name\", \"iot\"]\n",
                "    return features\n",
                "\n",
                "def generate_features(windowed_frame: pd.DataFrame):\n",
                "    window_groups = windowed_frame.groupby(\"window_id\", observed=True)\n",
                "\n",
                "    mean = window_groups['packet_size'].mean()\n",
                "    std = window_groups['packet_size'].std()\n",
                "    n_bytes = window_groups['packet_size'].sum()\n",
                "\n",
                "    # TODO Not sure if fillna is a good idea\n",
                "    return pd.concat([mean, std, n_bytes], axis=1).fillna(0)\n",
                "\n",
                "def _round_to(value, rounding=100.0):\n",
                "    return int(np.ceil(value / rounding)) * rounding\n",
                "\n",
                "def pad_data(windowed_frame, rounding=100.0):\n",
                "    padded = windowed_frame.copy()\n",
                "    padded['packet_size'] = padded['packet_size'].apply(lambda x: _round_to(x, rounding=rounding))\n",
                "    return padded\n",
                "\n",
                "def get_devices_in_window(windowed_frame: pd.DataFrame):\n",
                "    devices_in_window = pd.DataFrame()\n",
                "    \n",
                "    for window_id in windowed_frame.index.unique():\n",
                "        devices = windowed_frame.loc[window_id]['device_name']\n",
                "        # iot = windowed_frame.loc[window_id]['iot']\n",
                "\n",
                "        if not isinstance(devices, str):\n",
                "            devices = devices.unique() \n",
                "            \n",
                "        devices_in_window = pd.concat([devices_in_window, pd.DataFrame({\"window_id\": window_id, \"devices\": devices})])\n",
                "\n",
                "    return devices_in_window\n",
                "\n",
                "labeled_features = generate_features_labeled(windowed_frame, devices)\n",
                "labeled_features_padded = generate_features_labeled(windowed_frame, devices, padded=True)\n",
                "\n",
                "display(labeled_features_padded)\n",
                "display(labeled_features)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from enum import StrEnum\n",
                "\n",
                "class Balancing(StrEnum):\n",
                "    NONE = \"none\"\n",
                "    STRATIFIED = \"stratified\"\n",
                "    OVER_UNDER = \"over_under\"\n",
                "    OVER_UNDER_RUS = \"over_under_rus\"\n",
                "\n",
                "def _balance_over_under(X, y, rus=False, random_state: int | None = None):\n",
                "    oversampling = SMOTE(random_state=random_state)\n",
                "\n",
                "    # Resource [32] is actually really old (2009), NearMiss is a better alternative than RUS\n",
                "    # but I left it as an option\n",
                "    if rus:\n",
                "        undersampling = RandomUnderSampler(random_state=random_state)\n",
                "    else:\n",
                "        undersampling = NearMiss()\n",
                "\n",
                "    # first oversample, then undersample\n",
                "    X, y = oversampling.fit_resample(X, y)\n",
                "    X, y = undersampling.fit_resample(X, y)\n",
                "\n",
                "    return X, y\n",
                "\n",
                "def _get_models():\n",
                "    models = {\n",
                "        \"Knn\" : neighbors.KNeighborsClassifier(n_neighbors=5, weights=\"distance\"),\n",
                "        \"DT\"  : tree.DecisionTreeClassifier(),\n",
                "        \"RF\"  : ensemble.RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0),\n",
                "        # \"SVM\" : svm.SVC(),\n",
                "    }\n",
                "    params = {\n",
                "        \"Knn\" : \n",
                "            {'n_neighbors' : [1, 2, 3, 4, 5, 6, 7, 8, 9, 20, 30, 40, 100], 'weights' : [\"distance\", \"uniform\"]},\n",
                "        \"DT\" : {},\n",
                "        \"RF\" : {},\n",
                "    }\n",
                "    # I don't really see the advantage of using an ensemble balancing method in conjuction with the\n",
                "    # under- and oversample techniques. I can't find any indication in the paper the referenced [32]\n",
                "    # that it actually works. So, I'm just keeping this here.\n",
                "    models[\"Majority Voting\"] = ensemble.VotingClassifier\n",
                "    params[\"Majority Voting\"] = {'voting' : ['hard', 'soft']}\n",
                "\n",
                "    return models, params\n",
                "\n",
                "def _using_kfold(n_splits):\n",
                "    return n_splits > 1\n",
                "\n",
                "def _trained_model_name(scores):\n",
                "    return \"{}_{}_{}\".format(scores[\"model\"][-1], scores[\"label\"][-1], scores[\"balancing\"][-1])\n",
                "\n",
                "def _evaluate(model_name: str, accuracy: int, target_label: str, balancing: Balancing, scores={}):\n",
                "    if len(scores.keys()) == 0:\n",
                "        scores = {\"model\" : [], \"accuracy\" : [], \"label\" : [], \"balancing\" : []}\n",
                "\n",
                "    scores[\"model\"].append(model_name)\n",
                "    scores[\"accuracy\"].append(accuracy)\n",
                "    scores[\"label\"].append(target_label)\n",
                "    scores[\"balancing\"].append(balancing)\n",
                "\n",
                "    return scores\n",
                "\n",
                "def _custom_tuner(X_train, y_train, balancing=Balancing.NONE, target_label=None, n_splits=10, n_jobs=-1):\n",
                "    if n_splits == 1:\n",
                "        print(\"Cannot use hyper parameter tuning with less than 2 splits. Defaulting to 10.\")\n",
                "        n_splits = 10\n",
                "    if balancing == Balancing.STRATIFIED:\n",
                "        print(\"Warning: Stratified learning with hyper parameter tuning is effectively Balancing.NONE\")\n",
                "        \n",
                "    models, params = _get_models()\n",
                "    scores = {}\n",
                "    trained = {}\n",
                "\n",
                "    for (name, model) in models.items():\n",
                "        if name == \"Majority Voting\":\n",
                "            model = model(list(trained.items()))\n",
                "\n",
                "        tuning = model_selection.GridSearchCV(\n",
                "                model, \n",
                "                params[name], \n",
                "                scoring = \"accuracy\", \n",
                "                cv=n_splits,\n",
                "                n_jobs=n_jobs\n",
                "            )\n",
                "        tuning.fit(X_train, y_train)\n",
                "        scores = _evaluate(name, tuning.best_score_, target_label=target_label, balancing=balancing, scores=scores)\n",
                "        trained[_trained_model_name(scores)] = tuning.best_estimator_\n",
                "\n",
                "    return trained, pd.DataFrame(scores)\n",
                "\n",
                "\n",
                "def _custom_kfold(X_train, y_train, target_label=None, balancing=Balancing.NONE, kf=None):\n",
                "    models, _ = _get_models()\n",
                "    scores = {}\n",
                "    trained = {}\n",
                "\n",
                "    if kf is not None:\n",
                "        folds = [ (X_train.iloc[train], X_train.iloc[test], y_train.iloc[train], y_train.iloc[test]) for (train, test) in kf.split(X_train, y_train) ]\n",
                "    else:\n",
                "        X_train, X_test, y_train, y_test = model_selection.train_test_split(X_train, y_train)\n",
                "        folds = [ (X_train, X_test, y_train, y_test) ]\n",
                "    \n",
                "    for (name, model) in models.items():\n",
                "        if name == \"Majority Voting\":\n",
                "            model = model(list(trained.items()))\n",
                "\n",
                "        for (X_train, X_test, y_train, y_test) in folds:\n",
                "            model.fit(X_train, y_train)\n",
                "            accuracy = metrics.accuracy_score(y_test, model.predict(X_test))\n",
                "            scores = _evaluate(name, accuracy, target_label, balancing, scores=scores)\n",
                "            trained[_trained_model_name(scores)] = model\n",
                "\n",
                "    return trained, pd.DataFrame(scores)\n",
                "\n",
                "def train_and_test_classifiers(labeled_features, target_label, balancing=Balancing.NONE, n_splits=10, shuffle=False, random_state=None, tuned=False):\n",
                "    \"\"\"\n",
                "    Trains and tests a series of models on the given, labeled data\n",
                "    (Knn, RF, DT, SVM, Majority Voting)\n",
                "\n",
                "    When n_splits < 2, KFold is not used and the test set has a size of 25% of the whole dataset\n",
                "    \"\"\"\n",
                "    assert not (balancing == Balancing.STRATIFIED and not _using_kfold(n_splits)) # KFold cannot be disabled if using the stratified balancing strategy\n",
                "    \n",
                "    # If there is no balancing, we condsider the original dataset to be balanced\n",
                "    # thus, it is balanced by default\n",
                "    X_bal = labeled_features.iloc[:, 1:3]\n",
                "    y_bal = labeled_features.iloc[:][target_label]\n",
                "    X_ubal = None\n",
                "    y_ubal = None\n",
                "\n",
                "    kf = model_selection.KFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state) if _using_kfold(n_splits) else None\n",
                "\n",
                "    # If we're balancing the data itself, we should keep an unbalanced set (ubal) as testing set\n",
                "    if balancing != Balancing.NONE:\n",
                "        X_bal, X_ubal, y_bal, y_ubal = model_selection.train_test_split(X_bal, y_bal, test_size=.1)\n",
                "\n",
                "    if balancing == Balancing.OVER_UNDER:\n",
                "        X_bal, y_bal = _balance_over_under(X_bal, y_bal, random_state=random_state)\n",
                "    if balancing == Balancing.OVER_UNDER_RUS:\n",
                "        X_bal, y_bal = _balance_over_under(X_bal, y_bal, rus=True, random_state=random_state)\n",
                "    if balancing == Balancing.STRATIFIED:\n",
                "        kf = model_selection.StratifiedKFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
                "\n",
                "    if tuned:\n",
                "        trained, scores_bal = _custom_tuner(X_bal, y_bal, n_splits=n_splits, balancing=balancing, target_label=target_label)\n",
                "    else:\n",
                "        trained, scores_bal = _custom_kfold(X_bal, y_bal, kf=kf, balancing=balancing, target_label=target_label) \n",
                "\n",
                "    # If we haven't done balancing, the scores are good as they are\n",
                "    if balancing == Balancing.NONE:\n",
                "        return trained, scores_bal\n",
                "\n",
                "    # Otherwise, we should test on the unbalanced datasets\n",
                "    scores_ubal = {}\n",
                "    for (name, model) in trained.items():\n",
                "        simple_name = name.split(\"_\")[0]\n",
                "        accuracy = metrics.accuracy_score(y_ubal, model.predict(X_ubal))\n",
                "        scores_ubal = _evaluate(simple_name, accuracy, target_label, balancing, scores=scores_ubal)\n",
                "\n",
                "    return trained, pd.DataFrame(scores_ubal)\n",
                "\n",
                "def run_all_experiments(labeled_features, trainer):\n",
                "    # for the device classifier, we only consider iot devices\n",
                "    device_features = labeled_features.loc[labeled_features['iot'] == True]\n",
                "\n",
                "    experiments = [\n",
                "        # Balancing as done in the paper\n",
                "        trainer(labeled_features, 'iot', balancing=Balancing.OVER_UNDER_RUS, n_splits=1),\n",
                "        trainer(device_features, 'device_name', balancing=Balancing.STRATIFIED),\n",
                "\n",
                "        # Balancing strategies reversed\n",
                "        trainer(device_features, 'device_name', balancing=Balancing.OVER_UNDER_RUS, n_splits=1),\n",
                "        trainer(labeled_features, 'iot', balancing=Balancing.OVER_UNDER, n_splits=1),\n",
                "\n",
                "        # Balancing without RUS (with NearMiss)\n",
                "        trainer(device_features, 'device_name', balancing=Balancing.OVER_UNDER, n_splits=1),\n",
                "\n",
                "        # No balancing for either\n",
                "        trainer(labeled_features, 'iot', balancing=Balancing.NONE),\n",
                "        trainer(device_features, 'device_name', balancing=Balancing.NONE),\n",
                "    ]\n",
                "\n",
                "    # Return (trained models, scores)\n",
                "    return [ experiment[0] for experiment in experiments ], \\\n",
                "            pd.concat([ experiment[1] for experiment in experiments ])\n",
                "\n",
                "def train_and_test_tuned(*params, **named):\n",
                "    return train_and_test_classifiers(*params, **named, tuned=True)\n",
                "\n",
                "all_clf, all_scores = run_all_experiments(labeled_features, train_and_test_classifiers)\n",
                "all_clf_tuned, all_scores_tuned = run_all_experiments(labeled_features, train_and_test_tuned)\n",
                "\n",
                "display(all_scores.groupby([\"model\", \"label\", \"balancing\"]).mean())\n",
                "display(all_scores_tuned.groupby([\"model\", \"label\", \"balancing\"]).mean())\n",
                "\n",
                "for experiment in all_clf_tuned:\n",
                "    for (name, model) in experiment.items():\n",
                "        print(name, model.get_params())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def draw_performance_plot(scores):\n",
                "    sns.catplot(scores, x=\"model\", y=\"accuracy\", kind=\"bar\")\n",
                "    plt.ylim(0, 1)\n",
                "\n",
                "def draw_multiple_perf(scores):\n",
                "    assert \"balancing\" in scores.columns\n",
                "\n",
                "    sns.catplot(scores, x=\"model\", y=\"accuracy\", col=\"balancing\", row=\"label\", kind=\"bar\")\n",
                "    plt.ylim(0, 1)\n",
                "\n",
                "draw_multiple_perf(all_scores)\n",
                "draw_multiple_perf(all_scores_tuned)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn import metrics\n",
                "\n",
                "def test_realistic(iot_classifier, device_classifier, unlabeled_features, devices_in_window):\n",
                "    assert len(unlabeled_features) == len(devices_in_window)\n",
                "\n",
                "    # accuracy = metrics.accuracy_score(devices_in_window, iot_classifier.predict(unlabeled_features))\n",
                "    accuracy = metrics.accuracy_score(devices_in_window, device_classifier.predict(unlabeled_features))\n",
                "    print(\"Accuracy:\", accuracy)\n",
                " \n",
                "\n",
                "unlabeled_features = generate_features(windowed_frame)\n",
                "devices_in_window = get_devices_in_window(windowed_frame)\n",
                "\n",
                "test_realistic(iot_classifier, device_classifier, unlabeled_features, devices_in_window)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.2"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
